{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/cropped-SummerWorkshop_Header.png\">  \n",
    "\n",
    "<h1 align=\"center\">Day 1 Workshop 2, SWDB 2023 </h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "<p>\n",
    "    In a regression problem, we are given pairs of data points $(\\vec{x}_i, y_i)$ where $i \\in [1,N]$.  We want to develop a function $f(\\vec{x})$ such that $f(\\vec{x}_i)\\approx y_i$ for each pair of points in the data set.\n",
    "    </p>\n",
    "    <p>\n",
    "    The simplest regression problem is linear regression, in which we try to create the function $f$ by linearly combining a set of functions that act on the points $x$.\n",
    "\n",
    "$f(\\vec{x}_i) = \\sum_j w_j \\phi(\\vec{x}_i)$\n",
    "\n",
    "The functions $\\phi(\\vec{x})$ are chosen according to the analysis.  They are often called \"features\".  The coefficients $w_j$ are called \"weights\".\n",
    "\n",
    "You may be familiar with a version of linear regression where the functions $\\phi$ are chosen to be the identity and a constant.  When the input space is one dimensional this is:\n",
    "\n",
    "$f(x) = w x + b$\n",
    "</p>\n",
    "<p>\n",
    "    This problem is defined by an \"error function\", whose minimization tries to force the function $f$ to approximate the data points $y_i$ on the inputs $\\vec{x}_i$.  This error function is\n",
    "\n",
    "$E = \\frac{1}{2} \\sum_i \\left | y_i - f\\left ( \\vec{x}_i \\right ) \\right |^2 = \\frac{1}{2} \\sum_i \\left | y_i - \\sum_j w_j \\phi (\\vec{x}_i ) \\right |^2 $\n",
    "</p>\n",
    "<p>\n",
    "    This particular problem has an exact analytic solution that is easy to implement, but in this tutorial, we will look at how to perform regression using the `scikit-learn` Python package.  `scikit-learn` has many regression algorithms in common use built in, most of which do not have simple analytic solutions.  In addition, other packages have adopted the `scikit-learn` style interface.  One advantage of this is that multiple algorithms can be deployed with the same code.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "The `scikit-learn` website:  http://scikit-learn.org/stable/\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "We're going to provide a simple example with fake data and then use the same model to fit data from the Brain Observatory.  The model we fit will be an $n$th order polynomial of a single variable:\n",
    "</p>\n",
    "<p>\n",
    "$f_n(x) = \\sum_{i=0}^n a_i x^i$\n",
    "</p>\n",
    "<p>\n",
    "i.e.\n",
    "</p>\n",
    "<p>\n",
    "$f_1(x) = a_0 + a_1 x$\n",
    "</p>\n",
    "<p>\n",
    "$f_2(x) = a_0 + a_1 x + a_2x^2$\n",
    "</p>\n",
    "<p>\n",
    "$\\dots$\n",
    "</p>\n",
    "<p>\n",
    "First we generate some example data.  This will be the true model of the data.  (In a real problem, we won't know this function; we're trying to fit it.)\n",
    "</p>\n",
    "<p>\n",
    "$F(x) = \\text{sin}(2\\pi x)$\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.linspace(0,1.0, 100)\n",
    "\n",
    "def f_true(xt): \n",
    "    return np.sin(2.0*np.pi*xt)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x0, f_true(x0))\n",
    "ax.set_ylabel('f_true(x)')\n",
    "ax.set_xlabel('x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "Now we generate some 'data' from this function.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "\n",
    "x = np.sort(np.random.random(n))\n",
    "y = f_true(x) + 1.0*np.random.normal(size=n)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y, 'o')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_xlabel('x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "<h3>Fitting a first order polynomial.</h3>\n",
    "</p>\n",
    "<p>\n",
    "Before we fit any models, we need to separate the data into train, validate, and test sets.  This is so that we can train the model (train), perform model comparison (validate), and test the performance of the model (test).\n",
    "</p>\n",
    "<p>\n",
    "`scikit-learn` has a function we can use called `train_test_split`.  We use this function twice in order to generate a validation set.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_validate, y_train, y_validate = train_test_split(x, y, train_size=0.5)\n",
    "x_validate, x_test, y_validate, y_test = train_test_split(x_validate, y_validate, test_size=0.5)\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(15,2))\n",
    "ax[0].plot(x_train, y_train, 'o')\n",
    "ax[1].plot(x_validate, y_validate, 'o')\n",
    "ax[2].plot(x_test, y_test, 'o')\n",
    "\n",
    "ax[0].set_title('Train')\n",
    "ax[1].set_title('Validate')\n",
    "ax[2].set_title('Test')\n",
    "\n",
    "for i in range(3):\n",
    "    ax[i].set_ylim(-4, 4)\n",
    "    ax[i].set_xlabel('x')\n",
    "    ax[i].set_ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "In ordert to fit the model, we need to create the object that will manage the fitting.  We are going to use the `LinearRegression` model from `sklearn.linear_model`.  Fitting works by calling the `fit` method with the data.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression as LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "We reshape `x` in the `fit` method because it expects a two dimensional array of shape (samples, dimensions).\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LR()\n",
    "lr.fit(x_train.reshape(-1, 1), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "Now we compare the result to the validation set.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_validate, y_validate, 'o')\n",
    "ax.plot(x0.reshape(-1,1), lr.predict(x0.reshape(-1,1)), '-')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylim(-4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "That doesn't seem right.  Let's try multiple orders of polynomials so that we can compare them with the validation set.\n",
    "</p>\n",
    "<p>\n",
    "     We define a function `nth_polynomial` in order to create input data whose rows are data points and whose columns are the terms in the polynomial ($x$, $x^2$, $x^3$, $\\dots$)\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nth_polynomial(x, n):\n",
    "    return np.stack([x**i for i in range(1, n+1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_order = 9\n",
    "\n",
    "lr_list = [LR() for i in range(max_order)]\n",
    "for i, lr in enumerate(lr_list):\n",
    "    x_nth = nth_polynomial(x_train, i+1)\n",
    "    lr.fit(x_nth, y_train)\n",
    "    \n",
    "fig, ax = plt.subplots(3,3, figsize=(15,15))\n",
    "\n",
    "for i, lr in enumerate(lr_list):\n",
    "    xi = i%3\n",
    "    yi = i//3\n",
    "    x_nth = nth_polynomial(x0, i+1)\n",
    "    ax[yi, xi].plot(x_train, y_train, 'o')\n",
    "    ax[yi, xi].plot(x0, lr.predict(x_nth))\n",
    "    ax[yi, xi].set_xlabel('x')\n",
    "    ax[yi, xi].set_ylabel('y')\n",
    "    ax[yi, xi].set_title('order '+str(i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "Now we plot them against the validation set.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3,3, figsize=(15,15))\n",
    "\n",
    "for i, lr in enumerate(lr_list):\n",
    "    xi = i%3\n",
    "    yi = i//3\n",
    "    x_nth = nth_polynomial(x0, i+1)\n",
    "    ax[yi, xi].plot(x_validate, y_validate, 'o')\n",
    "    ax[yi, xi].plot(x0, lr.predict(x_nth))\n",
    "    ax[yi, xi].set_xlabel('x')\n",
    "    ax[yi, xi].set_ylabel('y')\n",
    "    ax[yi, xi].set_title('order '+str(i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "To be more precise, we compute the score, which for this model is the $R^2$ of the fit.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2_vals = []\n",
    "\n",
    "for i, lr in enumerate(lr_list):\n",
    "    x_nth = nth_polynomial(x_validate, i+1)\n",
    "    R2 = lr.score(x_nth, y_validate)\n",
    "    R2_vals.append(R2)\n",
    "\n",
    "order = np.arange(1,max_order+1)\n",
    "fig, ax = plt.subplots(1,3, figsize=(15,2))\n",
    "ax[0].plot(order, R2_vals)\n",
    "ax[0].set_ylabel('Validation $R^2$')\n",
    "ax[0].set_xlabel('polynomial order')\n",
    "\n",
    "best_model_index = np.argmax(R2_vals)\n",
    "lr_best = lr_list[best_model_index]\n",
    "\n",
    "ax[1].plot(x_validate, y_validate, 'o')\n",
    "x_nth = nth_polynomial(x0, best_model_index+1)\n",
    "ax[1].plot(x0, lr_best.predict(x_nth))\n",
    "ax[1].set_ylabel('y_validate')\n",
    "ax[1].set_xlabel('x_validate')\n",
    "\n",
    "\n",
    "ax[2].plot(x_test, y_test, 'o')\n",
    "ax[2].plot(x0, lr_best.predict(x_nth))\n",
    "ax[1].set_ylabel('y_test')\n",
    "ax[1].set_xlabel('x_test')\n",
    "\n",
    "print(\"Best model is order:  {}\".format(best_model_index+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "<h2>Cross validation</h2>\n",
    "<p>\n",
    "`scikit-learn` has facilities for making cross-validation quite simple.  Here we show performing cross_validation with the functions `cross_validate` and `KFold`.  `cross_validate` performs cross validation and returns a dictionary of scores over folds.  `KFold` provides an iterator that produces indices that split the data into train and test folds.\n",
    "</p>\n",
    "<p>\n",
    "In the following, we'll ignore the data set we labeled \"validate\" above.  This is just for simplicity, so I don't have extra code recreating data sets.  The data being used for validation here is part of the cross validation separation of the training set in the code below.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_mean_error = np.zeros_like(lr_list)\n",
    "\n",
    "for i, lr in enumerate(lr_list):\n",
    "    x_nth = nth_polynomial(x_train, i+1)\n",
    "    cv_dict = cross_validate(lr, x_nth, y_train, cv=4)\n",
    "    cv_mean_error[i] = np.mean(cv_dict['test_score'])\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(cv_mean_error)\n",
    "ax.set_ylabel('Cross Validation $<R^2>$')\n",
    "ax.set_xlabel('polynomial order')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "We can do the same analysis with `KFold`.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = KFold(n_splits=4)\n",
    "\n",
    "scores = np.zeros_like(lr_list)\n",
    "\n",
    "for i, lr in enumerate(lr_list):\n",
    "    scores_temp = []\n",
    "    for train, test in folds.split(x_train):\n",
    "        x_nth = nth_polynomial(x_train[train], i+1)\n",
    "        lr.fit(x_nth, y_train[train])\n",
    "        x_nth = nth_polynomial(x_train[test], i+1)\n",
    "        scores_temp.append(lr.score(x_nth, y_train[test]))\n",
    "    scores[i] = np.mean(scores_temp)\n",
    "        \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(scores)\n",
    "ax.set_ylabel('Cross Validation $<R^2>$')\n",
    "ax.set_xlabel('polynomial order')\n",
    "# ax.set_ylim(-0.25, 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "Now that we know the best model, we can apply it to the test data.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_index = np.argmax(scores)\n",
    "lr_best = lr_list[best_model_index]\n",
    "x_nth = nth_polynomial(x_train, best_model_index+1)\n",
    "lr_best.fit(x_nth, y_train)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_test, y_test, 'o')\n",
    "x_nth = nth_polynomial(x0, best_model_index+1)\n",
    "ax.plot(x0, lr_best.predict(x_nth))\n",
    "ax.set_ylabel('y_test')\n",
    "ax.set_xlabel('x_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #DFF0D8; \">\n",
    "    \n",
    "    <h3> Task 1 </h3>\n",
    "    \n",
    "    <p>\n",
    "    We chose the number of splits quite arbitrarily, assuming that it probably doesn't matter very much. Try repeating this cross-validation analysis after doubling the number of splits -- i.e., use 8 now instead of 4. Does the \"best model\" change?\n",
    "    </p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<h2>Modeling neural activity with regression</h2>\n",
    "<p>\n",
    "Now let's try a simple example with the Allen Brain Observatory Visual Coding Dataset. Even in the absence of a visual stimulus, neural activity within mouse visual cortex (measured e.g., with calcium fluorescence) continuosly fluctuates. In recent years it has become clear that these ongoing fluctuations often correlate with ongoing behavior of the animal, with running speed being a particularly strong correlate (e.g., Stringer et al. (2019) Science).\n",
    "\n",
    "We now have a means to study quantitatively assess the strength and reliability of this sort of relationship. Specifically, we can construct a regression model for neural activity (using dF/F) based upon the running speed of the animal.  \n",
    "\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "The next few cells get the running speed and events record for a neuron from the Visual Coding Two-Photon Dataset</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import os\n",
    "\n",
    "platstring = platform.platform()\n",
    "if ('Darwin' in platstring) or ('macOS' in platstring):\n",
    "    # macOS \n",
    "    data_root = \"/Volumes/Brain2023/\"\n",
    "elif 'Windows'  in platstring:\n",
    "    # Windows (replace with the drive letter of USB drive)\n",
    "    data_root = \"E:/\"\n",
    "elif ('amzn' in platstring):\n",
    "    # then on Code Ocean\n",
    "    data_root = \"/data/\"\n",
    "else:\n",
    "    # then your own linux platform\n",
    "    # EDIT location where you mounted hard drive\n",
    "    data_root = \"/media/$USERNAME/Brain2023/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allensdk.core.brain_observatory_cache import BrainObservatoryCache\n",
    "\n",
    "manifest_path= os.path.join(data_root,'allen-brain-observatory/visual-coding-2p/manifest.json')\n",
    "boc = BrainObservatoryCache(manifest_file=manifest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "expt_container_id = 637998953\n",
    "cell_id = 662191687\n",
    "\n",
    "expt_session_info = boc.get_ophys_experiments(experiment_container_ids=[expt_container_id])\n",
    "expt_session_info_df = pd.DataFrame(expt_session_info)\n",
    "expt_session_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id = expt_session_info_df[expt_session_info_df.session_type=='three_session_A'].id.iloc[0]\n",
    "data_set = boc.get_ophys_experiment_data(ophys_experiment_id=session_id)\n",
    "\n",
    "cell_idx = data_set.get_cell_specimen_indices([cell_id])[0]\n",
    "events = boc.get_ophys_experiment_events(ophys_experiment_id=session_id)#[cell_idx,:]\n",
    "events_time = data_set.get_fluorescence_timestamps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "Let's begin by plotting the cell's calcium trace alongside running speed.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx, dx_time = data_set.get_running_speed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(2,1,figsize=(8,8))\n",
    "ax1.plot(events_time, events[cell_idx,:], 'b') # need to transpose for plotting purposes\n",
    "ax1.set_xlabel(\"Time (s)\")\n",
    "ax1.set_ylabel(\"dF/F\")\n",
    "\n",
    "ax2.plot(dx_time, dx, 'k')\n",
    "ax2.set_xlabel(\"Time (s)\")\n",
    "ax2.set_ylabel(\"Running speed (cm/s)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "Looks like there may some relationship here, but it's really hard to tell -- the running speed fluctuations are hardly visible, as the scale has been automatically adjusted to accommodate what appears to be a very large outlier.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "If we were to model the relationship between these two variables without further processing, this one outlier could have a huge effect -- just as the outlier makes it hard to see everything else going on, the outlier's high amplitude will also contribute disproportionately to quantitative analyses that are based on variance. This is why it's crucial to visualize data!\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "Of course, we could manually delete this point; however, often these types of recordings have artifacts that last more than one data point, or appear more than a couple times during the experiment. Ideally, we would like to have a more systematic approach to account for these outliers.\n",
    "</p>\n",
    "\n",
    "<p>    \n",
    "Fortunately, running speed is sampled at a rate (30 Hz) that is quite a bit faster than we would expect to see meaningful changes in the locomotion behavior of the animal. This makes it reasonable to apply a \"median filter\" to this variable, which replaces each value with the median value of the surrounding number of points (the exact number is a parameter we can choose. Median filters are an excellent strategy when tasked with removing high amplitude outliers from a signal, as the median is an outlier-robust statistic.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "Let's give this another try below, plotting both the original and median-filtered running speed for comparison.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "\n",
    "dx_medfilt = signal.medfilt(dx,3) # replaces every value by the median taken over the surrounding 3 time points\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(2,1,figsize=(8,8))\n",
    "ax1.plot(events_time, events[cell_idx,:], 'b')\n",
    "ax1.set_xlabel(\"Time (s)\")\n",
    "ax1.set_ylabel(\"dF/f\")\n",
    "\n",
    "ax2.plot(dx_time, dx_medfilt, 'k')\n",
    "ax2.set_xlabel(\"Time (s)\")\n",
    "ax2.set_ylabel(\"Running speed (cm/s)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "Seems like it worked quite well! Let's move forward with the median filtered running speed then.\n",
    "</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = dx_medfilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = dx.shape[0]\n",
    "\n",
    "dx_train = dx[:L//2]\n",
    "dx_validate = dx[L//2:3*L//4]\n",
    "dx_test = dx[3*L//4:]\n",
    "\n",
    "events_train = events[:,:L//2]\n",
    "events_validate = events[:,L//2:3*L//4]\n",
    "events_test = events[:,3*L//4:]\n",
    "\n",
    "print((L, events.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(dx_train, events_train[cell_idx],'o', label='train')\n",
    "ax.plot(dx_validate, events_validate[cell_idx],'o',alpha=0.25, label='validate')\n",
    "ax.plot(dx_test, events_test[cell_idx],'o',alpha=0.25, label='test')\n",
    "ax.set_ylabel('Response (dF/f)')\n",
    "ax.set_xlabel('Running Speed (cm/s)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "There doesn't seem to be a particularly clear relationship between these variables. But, let's continue as we did above, splitting data into train, validate, and test epochs.\n",
    "</p>\n",
    "<p>\n",
    "To simplify things (for this analysis), let's go ahead and introduce a function to break up our data into bins -- we'll use this to bin our data according to timepoints sharing a similar running speed.    \n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bins an array \"a\" by averaging its values within bins whose limits are specified by \"bin_edges\"\n",
    "# Optionally include a list of arrays \"alt_array\" to be binned according to values of \"a\" \n",
    "\n",
    "def binning(a, bin_edges, alt_array=None):\n",
    "    n = len(bin_edges)-1\n",
    "    a_binned = np.zeros(n)\n",
    "    if alt_array is not None:\n",
    "        alt_binned_list = [np.zeros(n) for t in alt_array]\n",
    "    for i in range(n):\n",
    "        lower = bin_edges[i]\n",
    "        upper = bin_edges[i+1]\n",
    "        bin_mask = np.logical_and(a >= lower, a < upper)\n",
    "        a_masked = a[bin_mask]\n",
    "        if len(a_masked)>0:\n",
    "            a_binned[i] = np.mean(a_masked)\n",
    "        else:\n",
    "            a_binned[i] = 0\n",
    "        if alt_array is not None:\n",
    "            for j in range(len(alt_array)):\n",
    "                vals = alt_array[j][bin_mask]\n",
    "                if len(vals)>0:\n",
    "                    alt_binned_list[j][i] = np.mean(vals)\n",
    "                else:\n",
    "                    alt_binned_list[j][i] = 0\n",
    "            # alt_binned[i] = [np.mean(alt_array[bin_mask])\n",
    "        \n",
    "    if alt_array is not None:\n",
    "        return a_binned, alt_binned_list\n",
    "    else:\n",
    "        return a_binned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "plt.hist(dx)\n",
    "ax.set_xlabel('Running Speed (cm/s)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_edges = np.linspace(0,40,100) # few speed values >40; let's choose 100 speed bins from 0 to 40\n",
    "\n",
    "running_bin_train, events_bin_train = binning(dx_train, bin_edges, [events_train[cell_idx]])\n",
    "running_bin_validate, events_bin_validate = binning(dx_validate, bin_edges, [events_validate[cell_idx]])\n",
    "running_bin_test, events_bin_test = binning(dx_test, bin_edges, [events_test[cell_idx]])\n",
    "\n",
    "events_bin_train = events_bin_train[0]\n",
    "events_bin_validate = events_bin_validate[0]\n",
    "events_bin_test = events_bin_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(15,2))\n",
    "ax[0].plot(running_bin_train, events_bin_train, 'o')\n",
    "ax[1].plot(running_bin_validate, events_bin_validate, 'o')\n",
    "ax[2].plot(running_bin_test, events_bin_test, 'o')\n",
    "\n",
    "ax[0].set_title('Train')\n",
    "ax[1].set_title('Validate')\n",
    "ax[2].set_title('Test')\n",
    "\n",
    "for i in range(3):\n",
    "    ax[i].set_ylim(-0.01, 0.04)\n",
    "    ax[i].set_xlabel('Running Speed (cm/s)')\n",
    "    ax[i].set_ylabel('Response (dF/f)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_order1 = LR()\n",
    "lr_order1.fit(running_bin_train.reshape(-1,1), events_bin_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_order1.intercept_, lr_order1.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(running_bin_train, events_bin_train, 'o')\n",
    "ax.plot(running_bin_train.reshape(-1,1), lr_order1.predict(running_bin_train.reshape(-1,1)))\n",
    "ax.set_xlabel('Running Speed (cm/s)')\n",
    "ax.set_ylabel('Response (dF/f)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "Not bad! On the other hand, it does seem like we might be able to do even better if we allowed for a more nonlinear relationship. This is the elegance of the regression framework -- as above, we may build our design matrix with polynomials of different orders in order to model this apparently nonlinear relationship.\n",
    "</p>\n",
    "<p>\n",
    "BUT don't forget -- increasing model complexity runs the risk of decreasing generalizability (not to mention interpretability). This is a recurring theme that we will unpack more later on.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_order = 9\n",
    "\n",
    "lr_list = [LR() for i in range(max_order)]\n",
    "for i, lr in enumerate(lr_list):\n",
    "    running_nth_order = nth_polynomial(running_bin_train, i+1)\n",
    "    lr.fit(running_nth_order, events_bin_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3,3, figsize=(15,15))\n",
    "\n",
    "for i, lr in enumerate(lr_list):\n",
    "    xi = i%3\n",
    "    yi = i//3\n",
    "    running_nth_order = nth_polynomial(running_bin_train, i+1)\n",
    "    ax[yi, xi].plot(running_bin_train, events_bin_train, 'o')\n",
    "    ax[yi, xi].plot(running_bin_train, lr.predict(running_nth_order),'o')\n",
    "    ax[yi, xi].set_xlabel('Running Speed (cm/s)')\n",
    "    ax[yi, xi].set_ylabel('Response (dF/f)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "Indeed, looks like we get an even nicer fit via a nonlinear relationship. But, crucially, we need to test whether this generalizes.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3,3, figsize=(15,15))\n",
    "\n",
    "for i, lr in enumerate(lr_list):\n",
    "    xi = i%3\n",
    "    yi = i//3\n",
    "    ax[yi, xi].plot(running_bin_validate, events_bin_validate, 'o')\n",
    "    running_nth_order = nth_polynomial(running_bin_validate, i+1)\n",
    "    ax[yi, xi].plot(running_bin_validate, lr.predict(running_nth_order),'o')\n",
    "    ax[yi, xi].set_xlabel('Running Speed (cm/s)')\n",
    "    ax[yi, xi].set_ylabel('Response (dF/f)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2_vals = []\n",
    "\n",
    "for i, lr in enumerate(lr_list):\n",
    "    running_nth_order = nth_polynomial(running_bin_validate, i+1)\n",
    "    R2 = lr.score(running_nth_order, events_bin_validate)\n",
    "    R2_vals.append(R2)\n",
    "    \n",
    "order = np.arange(1,len(R2_vals)+1)\n",
    "    \n",
    "fig, ax = plt.subplots(1,3, figsize=(15,2))\n",
    "ax[0].plot(order, R2_vals)\n",
    "ax[0].set_ylabel('Validation $R^2$')\n",
    "ax[0].set_xlabel('polynomial order')\n",
    "\n",
    "best_model_index = np.argmax(R2_vals)\n",
    "lr_best = lr_list[best_model_index]\n",
    "\n",
    "ax[1].plot(running_bin_validate, events_bin_validate, 'o')\n",
    "running_nth_order = nth_polynomial(running_bin_validate, best_model_index+1)\n",
    "ax[1].plot(running_bin_validate, lr_best.predict(running_nth_order),'o')\n",
    "ax[1].set_xlabel('Running Speed (cm/s)')\n",
    "ax[1].set_ylabel('Response (dF/f)')\n",
    "ax[1].set_ylim(-0.1, 0.2)\n",
    "\n",
    "ax[2].plot(running_bin_test, events_bin_test, 'o')\n",
    "running_nth_order = nth_polynomial(running_bin_test, best_model_index+1)\n",
    "ax[2].plot(running_bin_test, lr_best.predict(running_nth_order),'o')\n",
    "ax[2].set_xlabel('Running Speed (cm/s)')\n",
    "ax[2].set_ylabel('Response (dF/f)')\n",
    "ax[2].set_ylim(-0.1, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_nth_order = nth_polynomial(running_bin_train, best_model_index+1)\n",
    "print(lr_best.score(running_nth_order, events_bin_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_nth_order = nth_polynomial(running_bin_test, best_model_index+1)\n",
    "print(lr_best.score(running_nth_order, events_bin_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(running_bin_test, events_bin_test, 'o')\n",
    "running_nth_order = nth_polynomial(running_bin_test, best_model_index+1)\n",
    "ax.plot(running_bin_test, lr_best.predict(running_nth_order),'o')\n",
    "ax.set_xlabel('Running Speed (cm/s)')\n",
    "ax.set_ylabel('Response (dF/f)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #DFF0D8; \">\n",
    "    \n",
    "    <h3> Task 2 </h3>\n",
    "    \n",
    "     How does our choice of running speed bin size affect the appearance of these plots? Repeat the analysis, choosing 10 bins instead of 100. Does your interpretation change?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<h2>Casting tuning curve prediction as a regression problem</h2>\n",
    "<p>The above procedures provide a powerful and generalizable framework for modeling observations as a function of one or more predictor variables. To illustrate this, we may now revisit our tuning curve modeling exercise from the morning session. We will start by formalizng this exercise using our new regression framework. We'll then see how this framework allows us to easily combine tuning curve and running speed predictors into a single predictive model.  \n",
    "    \n",
    "Before we get started, we must address one important difference between the regression problems we've discussed above, and the tuning curve regression that we'd like to perform. Specifically, our tuning curve predictions are based on a categorical rather than continuous variable (i.e., one of $8$ possible orientations). In other words, as above, we seek a model of the form:\n",
    "\n",
    "$$y = \\beta x+C,$$\n",
    "\n",
    "where $y$ is the calcium response, $X$ is now the stimulus information, and $\\beta$ and $C$ are constants. However, this $X$ is a categorical variable.\n",
    "\n",
    "One way to handle this would be to construct a separate model for each orientation:\n",
    "$$y = \\beta_1 X_1+C_1$$\n",
    "$$y = \\beta_2 X_2+C_2$$\n",
    "$$\\vdots$$\n",
    "$$y = \\beta_8 X_8+C_8 $$\n",
    "    \n",
    "Mathematically, this is cumbersome - we would need to look up which equation to use each time we want to predict new data. A more elegant alternative would be to combine predictors across orientations into a single model that simply operates piecewise:\n",
    "\n",
    "$$y = C+ \\begin{cases} \n",
    "\\beta_1 X * \\text{I}_1(X)  \\\\\n",
    "\\beta_2 X * \\text{I}_2(X) \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_8 X * \\text{I}_8(X)\n",
    "\\end{cases}$$\n",
    "    \n",
    "where $\\text{I}_n(X)$ is the <i>indicator function</i>:\n",
    "$$ \\text{I}_n(X) := \\begin{cases}\n",
    "1 \\text{ if } X=n, \\\\\n",
    "0 \\text{ else}\n",
    "\\end{cases} $$\n",
    "\n",
    "(Notice that this formulation merges the constants into one value, $C$. $C$ is, effectivly, the offset from zero for any model we fit.)\n",
    "\n",
    "Thus, as $X$ encodes the stimulus identity, $\\text{I}_n(X)$ determines which term in the equation we are operating with. This type of problem is called \"One-Hot\" encoding, because $X$ encodes what part of the equation is active. Practically speaking, we can implement this indicator function by creating a vector for each sample and setting $X_i = 1$ for whichever case is true. For example, if we had just two stimulus types, we might have: \n",
    "\n",
    "$$ X_1 = [1,0] $$\n",
    "$$ X_2 = [0,1] $$ \n",
    "\n",
    "Finally, if we have many observations, we can stack each of these $X$ observations to form a \"Design Matrix.\" \n",
    "\n",
    "We will have a corresponding fitting parameter vector, $$\\beta = [\\beta_1,\\beta_2,\\ldots,\\beta_8]$$\n",
    "\n",
    "Our whole problem can now be written: \n",
    "$$y = \\beta X$$ \n",
    "\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p> Let's begin by revisiting our tuning curve analysis.    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_index = 1 # resetting so we're all using the same model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_table = data_set.get_stimulus_table(stimulus_name='drifting_gratings')\n",
    "stim_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's organize our responses according to stimuli and cells, as we did this morning\n",
    "load_loc = os.path.join('/scratch','Workshop1')\n",
    "\n",
    "orientation = np.load(os.path.join(load_loc,str(session_id)+'_orientation.npy'))\n",
    "temp_freq = np.load(os.path.join(load_loc,str(session_id)+'_temp_freq.npy'))\n",
    "mean_response_all_cells = np.load(os.path.join(load_loc,str(session_id)+'_mean_response_all.npy'))\n",
    "\n",
    "# Get all orientations\n",
    "orientations = np.unique(orientation)\n",
    "\n",
    "#orientation = np.zeros((len(stim_table)))\n",
    "#temp_freq = np.zeros((len(stim_table)))\n",
    "#mean_response_all_cells = np.zeros((len(stim_table),events.shape[0]))\n",
    "\n",
    "#for ii in range(len(stim_table)):\n",
    "#    orientation[ii] = stim_table.orientation[ii]\n",
    "#    temp_freq[ii] = stim_table.temporal_frequency[ii]\n",
    "    \n",
    "#    for cc in range(events.shape[0]):\n",
    "#        this_response =  events[cc,stim_table.start[ii]:stim_table.start[ii]+60] # time course of the response for all cells (for this stimulus)\n",
    "#        mean_response_all_cells[ii,cc] = this_response.mean() # mean event occurrence over the 60 sec. time window for all cells (for this stimulus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p> As we did this morning, we'll just stick with just the temporal frequency we were looking at before.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orientation = orientation[temp_freq==2]\n",
    "mean_response_all_cells = mean_response_all_cells[temp_freq==2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orientations,ix = np.unique(orientation,return_inverse=True)\n",
    "print(orientations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p> Let's now try to recast this problem in the regression framework. The first step is to turn these stimuli into a design matrix that we can use to predict responses. Remember that the design matrix can be constructed in many different ways, e.g., by evaluating polynomials of varying degree for each of the orientations. For simplicity, here we'll just use \"identity functions\" -- making this a basic linear regression problem.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets build a matrix for every stimulus condition\n",
    "stims = np.vstack([orientation]).T\n",
    "\n",
    "# Each stimulus condition has a unique identity\n",
    "stimuli,stim_index,counts = np.unique(stims,axis=0,return_counts=True,return_inverse=True)\n",
    "\n",
    "# How many are there?\n",
    "print(len(stimuli))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the design matrix\n",
    "X = np.zeros((len(stim_index),len(stimuli)))\n",
    "for ii in range(len(stim_index)):\n",
    "    X[ii,stim_index[ii]] = 1\n",
    "    \n",
    "fig,ax = plt.subplots()\n",
    "ax.imshow(X,aspect='auto',interpolation='nearest')\n",
    "ax.set_xlabel('Stimulus orientation')\n",
    "ax.set_ylabel('Trial #')\n",
    "plt.title('Design matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "ax.imshow(mean_response_all_cells,aspect='auto')\n",
    "ax.set_xlabel('Cell #')\n",
    "ax.set_ylabel('Trial #')\n",
    "plt.title('Trial-averaged responses')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #DFF0D8; \">\n",
    "    \n",
    "    <h3> Task 3 </h3>\n",
    "    \n",
    "    <p>\n",
    "    Its hard to see a relationship between the two matrices by eye -- the design matrix isn't organized very clearly. Try sorting the design matrix rows based upon stimulus shown during that trial, so that trials having a similar orientation are grouped together (e.g., first few rows contain trials where the first orientation was shown, etc.)\n",
    "       </p> \n",
    "     <p>\n",
    "         <b>Hint</b>: Our design matrix has a single \"1\" or \"True\" value in each row. Recall \"np.argsort\" from our morning workshop! How can we order our rows in terms of where this \"1\" value lies?\n",
    "      </p> \n",
    "     <p>\n",
    "    Once you have the sorting and have applied to the design matrix (we should see some structure!), apply it also to the responses matrix. Does this help us visualize potential relationships? How would \"visually tuned\" cells appear in such a plot?\n",
    "    </p> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>Now, to split up our data once more. Recall that there are many ways to do this, with $k$-fold cross-validation being a particularly elegant strategy. For simplicity here, we will go back to our simple \"train-test\" split based on the first and second halves of the data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "mean_response = mean_response_all_cells[:,cell_idx]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, mean_response, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfmodel = LinearRegression(fit_intercept=False).fit(X_train, y_train,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scr_train = rfmodel.score(X_train,y_train) # R^2 (since this is a linear model)\n",
    "scr_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scr_test = rfmodel.score(X_test,y_test)\n",
    "scr_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "In general, we see that model performance is worse in test data as compared to training data. Of course, this is expected -- our model was optimized for the training data!\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "So, how we can improve model performance? One approach might be to include predictor variables that could aid in our ability to model neuronal responses. A common observation is that the behavioral state of the mouse -- in particular, whether or not is currently running -- can alter the responses of neurons to visual stimuli. We'll dig into this topic a bit more on Day 5. For now, let's just see if adding running information can improve our model performance.\n",
    "</p>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<h2>Multiple linear regression: combining stimulus orientation and running speed</h2>\n",
    "<p> We now have a precise quantification of how well tuning information predicts variability in the visual responses (as measured by calcium fluorescence). We now wish to see whether running speed can improve our model performance. Ideally, we'd like to systematically re-evaluate polynomial order in the context of tuning information, as the \"best order\" can change. In the interest of time, though, we'll stick with the best-performing model that we learned above.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "Let's start by (re-)examining how running speed varies over the course of the session.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load running speed\n",
    "dx, dx_time = data_set.get_running_speed()\n",
    "\n",
    "# We'll need to reapply our median filter\n",
    "dx = signal.medfilt(dx,3)\n",
    "\n",
    "# Let's plot running speed alongside the calcium trace we've been studying.\n",
    "fig, (ax1,ax2) = plt.subplots(2,1,figsize=(8,6))\n",
    "\n",
    "ax1.set_xlabel('Time (s)')\n",
    "ax1.set_ylabel('dF/F')\n",
    "ax1.plot(events_time, events[cell_idx,:], color='b')\n",
    "\n",
    "ax2.set_xlabel('Time (s)')\n",
    "ax2.set_ylabel('Running speed (cm/s)')\n",
    "ax2.plot(dx_time, dx, color='k')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "To get a better sense of how locomotion relates to the stimulus responses, let's start by splitting up trials into \"running\" and \"still\" and see whether our tuning curve changes.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spd = np.zeros((len(stim_table),60))\n",
    "for ii in range(len(stim_table)):\n",
    "    spd[ii,:] = dx_medfilt[stim_table.start[ii]:stim_table.start[ii]+60]\n",
    "\n",
    "spd = spd[temp_freq==2,:]\n",
    "\n",
    "mean_spd = spd.mean(axis=1)\n",
    "run_mask = mean_spd>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reconstruct tuning curve events, separating \"running\" vs. \"still\" trials\n",
    "tuning_run = np.zeros(orientations.shape)\n",
    "tuning_still = np.zeros(orientations.shape)\n",
    "for ii in range(orientations.shape[0]):\n",
    "    tuning_run[ii] = mean_response[(ix==ii) & run_mask].mean()\n",
    "    tuning_still[ii] = mean_response[(ix==ii) & ~run_mask].mean()\n",
    "    \n",
    "fig,ax = plt.subplots()\n",
    "ax.plot(orientations,tuning_run, 'o-', label='Running')\n",
    "ax.plot(orientations,tuning_still, 'o-', label='Still')\n",
    "ax.set_xticks(orientations)\n",
    "ax.set_xlim(-10,325)\n",
    "ax.set_xlabel(\"Direction\", fontsize=16)\n",
    "ax.set_ylabel(\"Mean # Events\", fontsize=16)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "Looks like direction tuning is more apparent when the mouse is still! Of course, this is simply an initial impression...it remains unclear whether this apparent effect is stronger than expected by chance, and if so, whether it generalizes well to unseen data. To answer these questions, we'll now look to incorporate running speed (as a continuous variable) into our regression model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by constructing a new design matrix, \"Xspd\", that now includes running speed.   \n",
    "\n",
    "mean_spd = spd.mean(axis=1).reshape(-1,1) # mean speed during each trial\n",
    "running_nth_order = np.squeeze(nth_polynomial(mean_spd, best_model_index+1))\n",
    "\n",
    "Xspd = np.concatenate((X,running_nth_order), axis=1) # add column for run speed predictor; for binarized, replace mean_spd w/ run_mask.reshape(-1,1)\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.imshow(Xspd,aspect='auto',interpolation='nearest')\n",
    "ax.set_xlabel('Predictor [Stim orientations & Run speed]')\n",
    "ax.set_ylabel('Trial #')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to update our train-test split with the new design matrix\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xspd, mean_response, test_size=0.5, random_state=42)\n",
    "\n",
    "rfmodel = LinearRegression(fit_intercept=False).fit(X_train, y_train,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scr_train = rfmodel.score(X_train,y_train)\n",
    "scr_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "Hm...just a slight improvement ~40% of variance explained by just stimulus information in the training data set.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scr_test = rfmodel.score(X_test,y_test)\n",
    "scr_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "<p> :( It seems that we're still not doing well on the test data points. There are a few potential reasons for this. <i>Overfitting</i> is a particularly general problem that we'll encounter repeatedly -- our model parameters have been fit well to the training data, but fail to generalize to unseen observations. This accuracy vs. generalizability trade-off is a standard issue encountered in machine learning problems.\n",
    "</p>\n",
    "\n",
    "<p>There are additional challenges at play here, however. Recall how our running plots indicated far more running during the test as opposed to training timepoints. These kinds of violations of <i>stationarity</i> (i.e., fixed mean and/or variance of a process over time) pose a major challenge to data analysis  particularly data from awake, behaving animals. In such cases, how can we perform our train/validation/test splits becomes highly consequential. </p>\n",
    "  <p>  \n",
    "Consider the following...</p>\n",
    "    <ul>\n",
    "        <li>How might our analysis change if we exclude the final part of the session, when the mouse started running more?</li>\n",
    "        <li>Would it matter if we swapped our train and test epochs?</li>\n",
    "        <li>If we instead assigned every other time point to train or test data, such that training and test points are scattered throughout, would that fix the problem?</li>\n",
    "        <ul>\n",
    "            <li>What assumptions are we making in doing this?</li>\n",
    "        </ul>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
